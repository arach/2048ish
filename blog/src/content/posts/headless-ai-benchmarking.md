---
title: 'Headless Agent Benchmarking: Statistical Performance Analysis'
pubDate: 2025-07-26
description: 'Building a fast, headless testing framework to statistically compare algorithmic agent performance - discovering which strategies actually work best.'
---

Moving beyond UI-driven testing, we built a comprehensive headless benchmarking system to get real statistical data on algorithmic agent performance. The results were eye-opening.

## The Problem with UI Testing

While our interactive agents were fun to watch, we needed hard data:
- Visual testing is slow and subjective
- Sample sizes were too small for statistical significance  
- No way to compare strategies fairly across hundreds of games
- Performance bottlenecked by animations and rendering

We needed to go headless.

## Building the Test Harness

### Headless Game Engine

First, we stripped out all UI dependencies to create a pure game logic engine:

```typescript
export class HeadlessGame {
  private state: GameState;
  private random: RandomGenerator;
  private moveCount: number = 0;

  constructor(config: HeadlessGameConfig = {}) {
    this.random = config.seed !== undefined 
      ? new SeededRandomGenerator(config.seed)
      : new DefaultRandomGenerator();
    this.state = this.initializeGame(config.gridSize || 4);
  }

  public makeMove(direction: Direction): boolean {
    if (this.state.isGameOver) return false;
    
    const moveResult = this.simulateMove(this.state.grid, direction);
    if (!moveResult.hasChanged) return false;

    // Update state and add random tile
    this.state.grid = moveResult.newGrid;
    this.state.score += moveResult.scoreIncrease;
    this.moveCount++;
    
    const tileResult = addRandomTile(this.state.grid, this.random);
    this.state.grid = tileResult.grid;
    
    this.state.isGameOver = this.isGameOver();
    return true;
  }
}
```

Key optimizations:
- **Seeded randomness** for reproducible tests
- **No animations** or DOM manipulation  
- **Pure function** move simulation
- **Batch processing** of multiple games

### Benchmarking Framework

Then we built a statistical analysis framework:

```typescript
export class AgentBenchmark {
  async runBenchmark(config: BenchmarkConfig): Promise<BenchmarkComparison> {
    const results: StrategyStats[] = [];

    for (const strategy of this.strategies) {
      const stats = await this.benchmarkStrategy(strategy, config);
      results.push(stats);
    }

    return {
      strategies: results,
      summary: this.generateSummary(results),
      runConfig: config
    };
  }

  private calculateStats(name: string, results: SimulationResult[]): StrategyStats {
    const scores = results.map(r => r.score);
    const moves = results.map(r => r.moves);
    const maxTiles = results.map(r => r.maxTile);

    return {
      name,
      runs: results.length,
      avgScore: Math.round(this.average(scores)),
      scoreStdDev: Math.round(this.standardDeviation(scores, avgScore)),
      maxTileDistribution: this.getDistribution(maxTiles),
      // ... more stats
    };
  }
}
```

## Strategy Adapters

We created adapters to test existing AI strategies in the headless environment:

```typescript
export const greedyStrategy: BenchmarkStrategy = {
  name: "Greedy",
  description: "Always makes the move that creates the most merges immediately",
  strategy: (game: HeadlessGame): Direction | null => {
    const greedy = new GreedyStrategy();
    const gameState = convertGameState(game);
    return greedy.getNextMove(gameState);
  }
};
```

## The Results: 100-Game Statistical Analysis

Running 100 games per strategy revealed clear performance differences:

### ðŸ† Performance Rankings

**By Average Score:**
1. **Greedy** - 3,251 points (best performer)
2. **Corner** - 2,508 points (solid and consistent)  
3. **Empty Cells** - 2,378 points (good performance)
4. **High Score** - 2,247 points (moderate performance)
5. **Random** - 1,058 points (baseline)
6. **Monotonic** - 837 points (needs refinement)

### ðŸ“Š Key Insights

**Greedy Strategy Dominance:**
- 3x better than random baseline
- 17% of games reach 512 tile
- 52% reach 256 tile  
- Most moves per game (269 avg)

**Corner Strategy Reliability:**
- Consistent performance with low variance
- 52% reach 256 tile (same as Greedy)
- Only 4% reach 512 tile (vs 17% for Greedy)
- Good balance of score and efficiency

**Surprising Results:**
- **Monotonic** strategy performed poorly (837 avg score)
- **High Score** strategy was slowest (3.32ms per game vs ~1ms)
- **Random** baseline achieved 1,058 points on average

### ðŸ“ˆ Statistical Confidence

With 100 games per strategy, we achieved solid statistical confidence:

```
Greedy: 3,251 Â± 315 points (95% confidence)
Corner: 2,508 Â± 206 points (95% confidence)  
Empty Cells: 2,378 Â± 214 points (95% confidence)
```

The performance differences are statistically significant.

## Technical Deep Dive

### Critical Bug Fix

During development, we discovered a critical bug in move validation:

```typescript
// BEFORE: Incorrect change detection
if (!hasChanged) {
  hasChanged = originalNumbers.length !== numbers.length ||
    originalNumbers.some((num, idx) => num !== numbers[idx]);
}

// AFTER: Compare original line to final line  
if (!hasChanged) {
  hasChanged = originalLine.some((cell, idx) => cell !== numbers[idx]);
}
```

This bug was causing valid moves to be rejected, leading to games ending after 0-1 moves.

### Performance Optimization

The headless engine achieved impressive performance:
- **0.56ms per game** (Random strategy)
- **1.1ms per game** (Greedy strategy)  
- **100 games in ~100ms** total runtime
- **10,000x faster** than UI-driven testing

## Running Your Own Benchmarks

The framework supports multiple benchmark modes:

```bash
# Quick test (25 games per strategy)
npm run benchmark:quick

# Standard analysis (100 games per strategy)  
npm run benchmark

# Comprehensive analysis (1000 games per strategy)
npm run benchmark:full
```

Results include:
- Score statistics (mean, median, std dev, confidence intervals)
- Move count analysis  
- Tile achievement distributions
- Performance timing
- Detailed JSON export for further analysis

## Future Improvements

Our testing framework opens up new possibilities:

1. **Hyperparameter Optimization** - Test strategy variations systematically
2. **Monte Carlo Tree Search** - Implement and benchmark advanced algorithms
3. **Genetic Algorithms** - Evolve better strategies through thousands of iterations
4. **A/B Testing** - Compare strategy tweaks with statistical rigor
5. **Tournament Mode** - Head-to-head strategy competitions

## Conclusion

Moving to headless testing transformed our understanding of algorithmic strategy performance. The **Greedy** strategy's clear dominance surprised us, while the **Monotonic** strategy's poor performance highlighted implementation issues.

Most importantly, we now have a robust framework for:
- âœ… Statistical significance with large sample sizes
- âœ… Reproducible, seeded testing  
- âœ… Performance optimization without UI bottlenecks
- âœ… Automated strategy comparison
- âœ… Confidence intervals and proper statistical analysis

The data doesn't lie - and now we have the tools to let our algorithmic agents prove themselves through rigorous statistical testing.

---

*The complete benchmarking framework is available in our codebase, with results automatically saved to JSON for further analysis.*